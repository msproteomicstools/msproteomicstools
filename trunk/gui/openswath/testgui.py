#!/usr/bin/python
# -*- coding: utf-8 -*-

import os

from AlignmentGUI import *
from models.MSData import RunDataModel

from msproteomicstoolslib.format.TransformationCollection import TransformationCollection
from msproteomicstoolslib.format.SWATHScoringReader import SWATHScoringReader
from msproteomicstoolslib.algorithms.alignment.AlignmentHelper import AlignmentExperiment as Experiment 

# options
REALIGN_RUNS = True
FDR_CUTOFF = 0.01
ONLY_SHOW_QUANTIFIED = False
ONLY_SHOW_QUANTIFIED = True

class SwathRun(object):
    """Data model for an individual SWATH injection, may contain multiple mzML files.

    This contains the model for all data from a single run (e.g. one panel in
    the viewer - in reality this could be multiple actual MS runs since in SRM
    not all peptides can be measured in the same injection or just multiple
    files generated by SWATH MS.
    """
    def __init__(self, files, runid=None):
        self.all_swathes = {}
        self.runid = runid
        self._in_memory = False
        self._files = files

        # extra info
        self._range_mapping = {}
        self._score_mapping = {}
        self._intensity_mapping = {}

        self._loadFiles(files)
        self._initialize()

    def _loadFiles(self, files):
        import pymzml
        for f in files:
            run_ = pymzml.run.Reader(f, build_index_from_scratch=True)
            run_.original_file = f
            first = run_.next()
            mz = first['precursors'][0]['mz']
            self.all_swathes[ int(mz) ] = RunDataModel(run_, f)

    def _initialize(self):
        """ A precursor can be mapped uniquely to a certain SWATH window.
        """
        self._precursor_run_map = {}
        self._sequences_mapping = {}
        self._chrom_id_run_map = {}
        for run_key, run in self.all_swathes.iteritems():
            for key in run._precursor_mapping:
                self._precursor_run_map[key] = run_key
            for key in run._sequences_mapping:
                tmp = self._sequences_mapping.get(key, [])
                tmp.extend( run._sequences_mapping[key] )
                self._sequences_mapping[key] = tmp

    def remove_precursors(self, toremove):
        for run_key, run in self.all_swathes.iteritems():
                # self._sequence_run_map[key] = run_key
            for key in toremove:
                run._precursor_mapping.pop(key, None)
                self._precursor_run_map.pop(key, None)
            run._group_precursors_by_sequence()

    #
    ## Getters (info)
    #
    def get_precursors_for_sequence(self, sequence):
        return self._sequences_mapping.get(sequence, [])

    def get_transitions_for_precursor(self, precursor):
        run = self._precursor_run_map.get( str(precursor), None)
        if run is None:
            return []
        return self.all_swathes[run].get_transitions_for_precursor(precursor)

    def get_all_precursor_ids(self):
        return self._precursor_run_map.keys()

    def get_all_peptide_sequences(self):
        res = set([])
        for m in self.all_swathes.values():
            res.update( m._sequences_mapping.keys() )
        return res

    def getAllSwathes(self):
        return self.all_swathes

    #
    ## Getters (data) -> see ChromatogramTransition.getData
    #
    def get_data_for_transition(self, chrom_id):
        raise Exception("Not implemented")

    def getTransitionCount(self):
        return sum([r.getTransitionCount() for r in self.all_swathes.values()] )

    def get_data_for_precursor(self, precursor):
        run = self._precursor_run_map[str(precursor)]
        return self.all_swathes[run].get_data_for_precursor(precursor)

    def get_range_data(self, precursor):
        return self._range_mapping.get(precursor, [0,0])

    def get_score_data(self, precursor):
        return self._score_mapping.get(precursor, None)

    def get_intensity_data(self, precursor):
        return self._intensity_mapping.get(precursor, None)

    def get_id(self):
        fileid = ""
        if len(self._files) > 0:
            fileid = os.path.basename(self._files[0]) 

        return self.runid + fileid

class SwathRunCollection(object):
    """A collection of SWATH files

    Contains multiple SwathRun objects which each represent one injection.
    """

    def __init__(self):
        self.swath_chromatograms = {}

    def initialize_from_directories(self, runid_mapping):
        """Initialize from a directory, assuming that all .mzML files in the
        same directory are from the same run.

        Requires a dictionary of form { run_id : directory }
        """
        self.swath_chromatograms = {}
        for runid, dname in runid_mapping.iteritems():
            import glob
            files = glob.glob(os.path.join(dname + "/*.mzML") )
            self.swath_chromatograms[ runid ] = SwathRun(files, runid)

    def initialize_from_files(self, filenames):
        """Initialize from individual files, setting the runid as increasing
        integers.  
        """
        self.swath_chromatograms = {}
        for i,f in enumerate(filenames):
            runid = i
            self.swath_chromatograms[ runid ] = SwathRun([f])

    def getSwathFiles(self):
        return self.swath_chromatograms.values()

    def getSwathFile(self, key):
        return self.swath_chromatograms[key]

    def getRunIds(self):
        return self.swath_chromatograms.keys()

class DataModelNew(DataModel):

    def __init__(self):
        super(DataModelNew, self).__init__()
        pass

    def loadFiles_with_peakgroups(self, trafo_filenames, aligned_pg_files):

        # Read the chromatograms
        swathfiles = SwathRunCollection()
        swathfiles.initialize_from_directories( dict( [ (d["id"], d["directory"]) for d in trafo_filenames] ) )
        self.runs = [run for run in swathfiles.getSwathFiles()]
        print "Find in total a collection of %s runs." % len(swathfiles.getRunIds() )

        self.read_trafo(trafo_filenames)
        self.read_peakgroup_files(aligned_pg_files, swathfiles)

    def read_trafo(self, trafo_filenames):
        # Read the transformations
        transformation_collection_ = TransformationCollection()
        for filename in [d["trafo_file"] for d in trafo_filenames]:
          transformation_collection_.readTransformationData(filename)
        transformation_collection_.initialize_from_data(reverse=True)

    def read_peakgroup_files(self, aligned_pg_files, swathfiles):

        # Read in the peakgroup files, parse them and map across runs
        reader = SWATHScoringReader.newReader(aligned_pg_files, "openswath", readmethod="complete")
        new_exp = Experiment()
        new_exp.runs = reader.parse_files(REALIGN_RUNS)
        multipeptides = new_exp.get_all_multipeptides(FDR_CUTOFF, verbose=False)

        # Build map of the PeptideName/Charge to the individual multipeptide
        peakgroup_map = {}
        for m in multipeptides:
            pg = m.find_best_peptide_pg()
            peakgroup_map[ pg.get_value("FullPeptideName") + "/" + pg.get_value("Charge")] = m

        for swathrun in swathfiles.getSwathFiles():
            if ONLY_SHOW_QUANTIFIED:
                intersection = set(swathrun.get_all_precursor_ids()).intersection( peakgroup_map.keys() )
                todelete = set(swathrun.get_all_precursor_ids()).difference(intersection)
                swathrun.remove_precursors(todelete)

            # for each precursor in this run, identify the best peakgroup and store the value
            for precursor_id in swathrun.get_all_precursor_ids():
                if not peakgroup_map.has_key(precursor_id): 
                    continue

                m = peakgroup_map[ precursor_id ]
                if m.has_peptide(swathrun.runid):
                    pg = m.get_peptide(swathrun.runid).get_best_peakgroup()
                    try:
                        swathrun._range_mapping[precursor_id]       = [ float(pg.get_value("leftWidth")), float(pg.get_value("rightWidth")) ]
                        swathrun._score_mapping[precursor_id]       = float(pg.get_value("m_score"))
                        swathrun._intensity_mapping[precursor_id]   = float(pg.get_value("Intensity"))
                    except Exception: 
                        pass
                    
    def load_from_yaml(self, yamlfile):

        import yaml
        data = yaml.load(open(yamlfile) )["AlignedSwathRuns"]
        alignment_files = data["PeakGroupData"]
        trafo_fnames = [d["trafo_file"] for d in data["RawData"]]
        self.loadFiles_with_peakgroups(data["RawData"], alignment_files)

class MainWindowNew(MainWindow):
    
    def __init__(self):
        super(MainWindowNew, self).__init__()
        
        self.c = Communicate()
        self.data_model = DataModelNew()

        self.initUI()

"""
Sample Yaml file:

AlignedSwathRuns:
  PeakGroupData: [/path/peakgroups.csv]
  RawData:
  - {directory: /path/Strep0_Repl1_R02, id: '0_0', trafo_file:  /path/Strep0_Repl1_R02/transformation-0_0-0_1.tr}
  - {directory: /path/Strep0_Repl2_R02, id: '0_1', trafo_file:  /path/Strep0_Repl2_R02/transformation-0_1-0_1.tr}
  - {directory: /path/Strep10_Repl1_R02, id: '0_2', trafo_file: /path/Strep10_Repl1_R02/transformation-0_2-0_1.tr}
  - {directory: /path/Strep10_Repl2_R02, id: '0_3', trafo_file: /path/Strep10_Repl2_R02/transformation-0_3-0_1.tr}
  ReferenceRun: '0_1'
"""

app = QtGui.QApplication(sys.argv)
ex = MainWindowNew()
ex.data_model.load_from_yaml("/home/hr/projects/msproteomicstools/test100align.yaml")
# ex.data_model.load_from_yaml("/home/hr/projects/msproteomicstools/allAlign.yaml")
ex._refresh_view()
sys.exit(app.exec_())



